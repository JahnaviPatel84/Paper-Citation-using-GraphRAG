Doi,Publishing Date,Title,Authors,URL for pdf,
"
10.48550/arXiv.2311.01282","Nov 3, 2023",FlashDecoding++: Faster Large Language Model Inference on GPUs,"Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, Yu Wang",https://arxiv.org/pdf/2311.01282,Inference
"
10.48550/arXiv.2311.18677
","Nov 30, 2023",Splitwise: Efficient generative LLM inference using phase splitting,"Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, Ricardo Bianchini",https://arxiv.org/pdf/2311.18677,Inference
"10.48550/arXiv.2312.07104
","Dec 12, 2023","SGLang: Efficient Execution of Structured Language Model Programs
","Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, Ying Sheng",https://arxiv.org/pdf/2312.07104,Inference
"
10.48550/arXiv.2403.19887","Mar 28, 2024",Jamba: A Hybrid Transformer-Mamba Language Model,"Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, Yoav Shoham",https://arxiv.org/pdf/2403.19887,Training
"
10.48550/arXiv.2312.00752
","Dec 1, 2023","Mamba: Linear-Time Sequence Modeling with Selective State Spaces
","Albert Gu, Tri Dao
",https://arxiv.org/pdf/2312.00752,Training
"
10.48550/arXiv.2404.02015
","Apr 2, 2024",MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving,"Jiangfei Duan, Runyu Lu, Haojie Duanmu, Xiuhong Li, Xingcheng Zhang, Dahua Lin, Ion Stoica, Hao Zhang",https://arxiv.org/pdf/2404.02015,Inference
"10.48550/arXiv.2311.03285
","Nov 6, 2023",S-LoRA: Serving Thousands of Concurrent LoRA Adapters,"Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica",https://arxiv.org/pdf/2311.03285,Inference
"10.48550/arXiv.2402.02057
","Feb 3, 2024","Break the Sequential Dependency of LLM Inference Using Lookahead Decoding
","Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang",https://arxiv.org/abs/2402.02057,Inference
"
10.48550/arXiv.2401.10774","Jan 19, 2024",Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads,"Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao",https://arxiv.org/pdf/2401.10774,Inference
"10.48550/arXiv.2211.17192
","Nov 30, 2022",Fast Inference from Transformers via Speculative Decoding,"Yaniv Leviathan, Matan Kalman, Yossi Matias",https://arxiv.org/pdf/2211.17192,Inference
10.48550/arXiv.2405.19888,"May 30, 2024",Parrot: Efficient Serving of LLM-based Applications with Semantic Variable,"Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu",https://arxiv.org/pdf/2405.19888,Inference
"
10.18653/v1/2024.acl-long.681","Apr 25, 2024",LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding,"Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu",https://arxiv.org/pdf/2404.16710,Inference
"
10.48550/arXiv.2402.01869","Feb 2, 2024",InferCept: Efficient Intercept Support for Augmented Large Language Model Inference,"Reyna Abhyankar, Zijian He, Vikranth Srivatsa, Hao Zhang, Yiying Zhang",https://arxiv.org/pdf/2402.01869,Inference
"10.48550/arXiv.2401.00588
","Dec 31, 2023",Fairness in Serving Large Language Models,"Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang Zhuo, Joseph E. Gonzalez, Ion Stoica",https://arxiv.org/pdf/2401.00588,Inference
"10.48550/arXiv.2401.09670
","Jan 18, 2024",DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving,"Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, Hao Zhang
",https://arxiv.org/pdf/2401.09670,Inference
"10.48550/arXiv.2012.12877
","Jan 15, 2021","Training data-efficient image transformers & distillation through attention","Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou",https://arxiv.org/pdf/2012.12877,Training
"10.48550/arXiv.2004.05150
","Dec 2, 2020","Longformer: The Long-Document Transformer​","Iz Beltagy, Matthew E. Peters, Arman Cohan",https://arxiv.org/pdf/2004.05150,Training
10.48550/arXiv.2006.04768,"Jun 14, 2020",Linformer: Self-Attention with Linear Complexity,"Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma",https://arxiv.org/pdf/2006.04768,Training
"10.48550/arXiv.1910.10683
","Sep 19, 2023","Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
","Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",https://arxiv.org/pdf/1910.10683,Training
"10.48550/arXiv.2303.06296
","Jul 25, 2023",Stabilizing Transformer Training by Preventing Attention Entropy Collapse,"Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, Josh Susskind",https://arxiv.org/pdf/2303.06296,Training
10.48550/arXiv.2404.0799,"Apr 7, 2024",A Multi-Level Framework for Accelerating Training Transformer Models​,"Longwei Zou, Han Zhang, Yangdong Deng",https://arxiv.org/pdf/2404.07999,Training
"10.48550/arXiv.2303.05295
","Mar 9, 2023",Dynamic Stashing Quantization for Efficient Transformer Training,"Guo Yang, Daniel Lo, Robert Mullins, Yiren Zhao",https://arxiv.org/pdf/2303.05295,Training
"10.48550/arXiv.2503.04598
","Mar 6, 2025",HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization,"Zhijian Zhuo, Yutao Zeng, Ya Wang, Sijun Zhang, Jian Yang, Xiaoqing Li, Xun Zhou, Jinwen Ma",https://arxiv.org/pdf/2503.04598,Training
10.48550/arXiv.2307.03170,"Jul 6, 2023",Focused Transformer: Contrastive Training for Context Scaling,"Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś",https://arxiv.org/pdf/2307.03170,Training
"10.48550/arXiv.2205.10505
","May 21, 2022",A Study on Transformer Configuration and Training Objective,"Fuzhao Xue, Jianghai Chen, Aixin Sun, Xiaozhe Ren, Zangwei Zheng, Xiaoxin He, Yongming Chen, Xin Jiang, Yang You",https://arxiv.org/pdf/2205.10505,Training
10.48550/arXiv.1901.02860,"Jun 2, 2019",Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context​,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",https://arxiv.org/pdf/1901.02860,Training
"10.48550/arXiv.1906.08237
","Jan 2, 2020",XLNet: Generalized Autoregressive Pretraining for Language Understanding​,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le",https://arxiv.org/pdf/1906.08237,Training
"10.48550/arXiv.2205.14135
","Jun 23, 2022",FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,"Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré
",https://arxiv.org/pdf/2205.14135,
10.48550/arXiv.2307.08691,"Jul 17, 2023",FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning,Tri Dao,https://arxiv.org/pdf/2307.08691,
"10.48550/arXiv.2305.13245
","May 22, 2023","GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
","Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai
",https://arxiv.org/pdf/2305.13245,
"10.48550/arXiv.2407.08608
","Jul 12, 2024","FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision
","Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao",https://arxiv.org/pdf/2407.08608,
10.48550/arXiv.2106.09685,"Oct 16, 2021","LoRA: Low-Rank Adaptation of Large Language Models
","Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen
",https://arxiv.org/pdf/2106.09685,
10.48550/arXiv.2305.14314,"May 23, 2023","QLoRA: Efficient Finetuning of Quantized LLMs
","Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer",https://arxiv.org/pdf/2305.14314,
"10.48550/arXiv.2104.08691
","Sep 2, 2021","The Power of Scale for Parameter-Efficient Prompt Tuning
","Brian Lester, Rami Al-Rfou, Noah Constant",https://arxiv.org/pdf/2104.08691,
10.48550/arXiv.2101.00190,"Jan 1, 2021","Prefix-Tuning: Optimizing Continuous Prompts for Generation
","Xiang Lisa Li, Percy Liang",https://arxiv.org/pdf/2101.00190,
10.48550/arXiv.2402.09353,"Feb 14, 2024","DoRA: Weight-Decomposed Low-Rank Adaptation
","Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Min-Hung Chen",https://arxiv.org/pdf/2402.09353,
10.48550/arXiv.2205.05638,"May 11, 2022","Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
","Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin Raffel
",https://arxiv.org/pdf/2205.05638,
"10.48550/arXiv.1811.06965
","Nov 16, 2018","GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
","Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen",https://arxiv.org/pdf/1811.06965,
"10.48550/arXiv.1909.08053
","Sep 17, 2019","Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
","Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",https://arxiv.org/pdf/1909.08053,
10.48550/arXiv.1910.02054,"May 13, 2020","ZeRO: Memory Optimizations Toward Training Trillion Parameter Models
","Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He",https://arxiv.org/pdf/1910.02054,
"10.48550/arXiv.2006.15704
","Jun 28, 2020","PyTorch Distributed: Experiences on Accelerating Data Parallel Training
","Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, Soumith Chintala",https://arxiv.org/pdf/2006.15704,
"10.48550/arXiv.2104.04473
","Apr 9, 2021","Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
","Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia
",https://arxiv.org/pdf/2104.04473,
"10.48550/arXiv.2111.04007
","Nov 7, 2021","Varuna: Scalable, Low-cost Training of Massive Deep Learning Models
","Sanjith Athlur, Nitika Saran, Muthian Sivathanu, Ramachandran Ramjee, Nipun Kwatra",https://arxiv.org/pdf/2111.04007,
10.48550/arXiv.2201.12023,"Jun 28, 2022","Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning
","Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica",https://arxiv.org/pdf/2201.12023,
10.48550/arXiv.2407.21783,"Nov 23, 2024","The Llama 3 Herd of Models
","Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia et al. (460 additional authors not shown)",https://arxiv.org/abs/2407.21783,
"10.48550/arXiv.2201.05596
","Jul 21, 2022","DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale
","Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He",https://arxiv.org/pdf/2201.05596,
10.48550/arXiv.2206.03382,"Jun 7, 2022","Tutel: Adaptive Mixture-of-Experts at Scale
","Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, Yongqiang Xiong",https://arxiv.org/pdf/2206.03382,
10.48550/arXiv.2006.16668,"Jun 30, 2020","GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding
","Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",https://arxiv.org/pdf/2006.16668,