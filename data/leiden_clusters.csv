Node ID,Cluster,validation_methods,advantages,updated_at,description,name,usage_in_paper,created_at,original_source,limitations,type,key_innovations
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:17,0,['Results on RACE dataset'],"['Implements layer normalization to achieve increased performance', 'Combines BERT and GPT-2 architectures']",2025-03-12T07:54:17.561000000+00:00,A pre-trained language model,BERT,Referenced as a benchmark model,2025-02-24T01:58:41.630000000+00:00,"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)",[],Model,"['Large transformer model with billions of parameters', 'Achieves SOTA results on RACE dataset']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:0,0,"['Experiments on two machine translation tasks (WMT 2014 English-to-German and WMT 2014 English-to-French)', 'Improvement in BLEU score']","['Superior quality in sequence transduction tasks', 'More parallelizable and requiring significantly less time to train']",2025-03-12T08:04:07.808000000+00:00,A widely used architecture for language models.,Transformer,Referenced as a baseline architecture.,2025-02-23T06:51:17.743000000+00:00,Attention Is All You Need (Vaswani et al.),[],architecture,"['Replacing recurrence and convolutions with attention mechanisms', 'Introducing self-attention and multi-head attention']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:1,0,"['Experiments on machine translation tasks', 'Improvement in BLEU score']","['Improved performance on sequence transduction tasks', 'More parallelizable']",2025-02-26T18:51:48.632000000+00:00,Attention mechanism for relating different positions in a sequence,Self-attention,Implemented in both BERT and Transformer architectures,2025-02-23T06:51:17.743000000+00:00,Attention Is All You Need (Vaswani et al.),[],mechanism,"['Scaling dot-product attention', 'Multi-head attention']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:110,6,"['Empirical results on 20 tasks', 'Comparison with BERT']",['Outperforms BERT on 20 tasks'],,Generalized autoregressive pretraining method,XLNet,,2025-03-12T07:47:42.685000000+00:00,,[],pretraining_method,"['Bidirectional context learning', 'Autoregressive formulation']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:18,0,,,2025-02-25T05:02:24.876000000+00:00,A type of neural network architecture used for natural language processing tasks,Transformer Architecture,Used as the base architecture for the BERT model,2025-02-24T01:58:41.715000000+00:00,Vaswani et al. (2017),,neural_network_architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:19,0,,,2025-02-25T05:02:24.876000000+00:00,An attention mechanism where each token can attend to every other token,Self-Attention Mechanism,Used in the original Transformer Architecture,2025-02-24T01:58:41.715000000+00:00,Vaswani et al. (2017),,attention_mechanism,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:35,0,,,,Uses task-specific architectures and pre-trained representations as features,ELMo,Compared to BERT's pre-training strategy in the paper,2025-02-26T18:51:48.632000000+00:00," Peters et al., 2018a)",,language representation model,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:36,0,,,,Uses minimal task-specific parameters and ne-tuning all pre-trained parameters for downsteam tasks,GPT,Compared to BERT's pre-training strategy in the paper,2025-02-26T18:51:48.632000000+00:00,"Radford et al., 2018)",,ne-tuning approach,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:44,0,"['experiments with large sparse models', 'comparison with dense models']","['increased pre-training speed', 'lower computational costs', 'reduced memory usage']",,a novel attention mechanism that extends the original Transformer architecture,Switch Transformer,,2025-02-26T18:59:36.296000000+00:00,,"['training instability', 'communication costs', 'complexity']",attention mechanism,"['simplified sparse routing algorithm', 'reduced communication and computational costs']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:102,0,['Experimental results on training iterations with 1 trillion parameters'],"['Improved throughput (10%+)', 'Comparable memory footprint to existing approaches']",2025-03-12T07:51:21.571000000+00:00,Large-scale language model for efficient training on GPU clusters,Megatron-LM,,2025-03-12T07:47:12.559000000+00:00,,['Up to 10% overhead due to pipelining schedule'],model architecture,"['Interleaved pipelining schedule', 'Tensor and pipeline parallelism', 'Data parallelism']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:103,0,['Results on WikiText103 and LAMBADA datasets'],"['Demonstrates improved performance on NLP tasks', 'Covers a wide range of NLP tasks such as article completion, question answering']",2025-03-12T07:52:42.052000000+00:00,A state-of-the-art model with 1.5 billion parameters,GPT-2,Cited as an example of a large model,2025-03-12T07:47:12.559000000+00:00,GPT-2: Language Models for Language Generation,[],deep learning model,"['Large transformer model with 8.3 billion parameters', 'Achieves SOTA results on WikiText103 and LAMBADA datasets']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:293,0,['Various architectural decisions'],"['High throughput', 'Small memory footprint', 'State-of-the-art performance on language model benchmarks and long-context evaluations.']",,A hybrid Transformer-Mamba mixture-of-experts (MoE) architecture.,Jamba,,2025-03-12T08:04:07.801000000+00:00,,[],architecture,"['Interleaves blocks of Transformer and Mamba layers', 'Adds MoE in some layers to increase model capacity while keeping active parameter usage manageable.']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:294,0,[],['Improved performance and higher throughput compared to Transformer'],,A recent state-space model.,Mamba,,2025-03-12T08:04:07.801000000+00:00,,[],model family,[]
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:3,0,,,2025-02-23T07:11:12.872000000+00:00,Complex neural networks with an encoder and a decoder,Recurrent neural networks,Discussed as previous approaches in sequence modeling,2025-02-23T06:51:17.867000000+00:00, Various papers,,architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:45,0,,,,a model architecture that selects different parameters for each incoming example,Mixture of Experts (MoE),"described as the inspiration for the Switch Transformer, but also mentioned as having limitations and complexities",2025-02-26T18:59:36.304000000+00:00,not specified in the paper,,model architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:159,0,,,,Natural Language Processing (NLP) models using self-attention mechanisms,Transformer-based Language Models,Baseline models for comparison with Megatron-LM,2025-03-12T07:51:21.575000000+00:00,Vaswani et al. (2017),,model architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:165,1,['Training massive models on 5x cheaper spot VMs while maintaining high training throughput'],"['Cost-effective compared to hyper-clusters', 'Improves end-to-end training time by up to 18x compared to model-parallel approaches and up to 26% compared to pipeline parallel approaches']",,A new system for training massive deep learning models on commodity networking,Varuna,,2025-03-12T07:52:42.046000000+00:00,,['Limited scalability compared to hyper-clusters'],system,"['Thrifty use of networking resources', 'Automatic configuration of training jobs to efficiently use commodity resources']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:2,0,"['Experiments on machine translation tasks', 'Improvement in BLEU score']","['Improved performance on sequence transduction tasks', 'More robust to changes in input sequence']",2025-02-23T07:11:12.790000000+00:00,Extension of self-attention mechanism with multiple attention heads,Multi-head attention,,2025-02-23T06:51:17.743000000+00:00,,[],mechanism,"['Using multiple attention heads', 'Learning attention weights for each head']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:94,6,[],"['Captures longer-term dependency', 'Resolves context fragmentation problem', 'Achieves better performance on both short and long sequences']",2025-03-12T07:47:42.690000000+00:00,State-of-the-art autoregressive model,Transformer-XL,Reference for XLNet's integration with its ideas,2025-03-12T07:46:49.293000000+00:00,"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)",[],model,"['Segment-level recurrence mechanism', 'Novel positional encoding scheme']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:22,1,"['Evaluate performance in few-shot setting', 'Test on various NLP datasets']","['Reaches competitiveness with prior state-of-the-art fine-tuning approaches', 'Demonstrates strong performance on various NLP datasets']",2025-03-12T07:55:58.546000000+00:00,Large autoregressive language model,GPT-3,Referenced as an example of a large autoregressive model,2025-02-25T05:02:52.246000000+00:00,Brown et al. (2020),['Faces methodological issues related to training on large web corpora'],model,['Scaling up language models to achieve few-shot learning']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:166,1,,,,A state-of-the-art model for natural language understanding with 340 million parameters,BERT-large,Cited as an example of a large model,2025-03-12T07:52:42.052000000+00:00,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,,deep learning model,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:167,1,,,,A state-of-the-art model with 8 billion parameters,Megatron,Cited as an example of a large model,2025-03-12T07:52:42.052000000+00:00,Megatron: Training Multi-Billion-Parameter Language Models Using Model Parallelism,,deep learning model,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:168,1,,,,A state-of-the-art model with 17 billion parameters,Turing-NLG,Cited as an example of a large model,2025-03-12T07:52:42.052000000+00:00,Turing-NLG: A 17 Billion Parameter Language Model,,deep learning model,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:180,1,['Comparative evaluation with hand-tuned model-parallel training systems'],"['Accelerates ML research and production by enabling model developers to quickly explore new model designs', 'Improves training performance by reducing the need for hand-tuning parallelization strategies']",,Automates model-parallel training of large deep learning (DL) models,Alpa,,2025-03-12T07:53:25.870000000+00:00,,['Requires strong machine learning (ML) and system expertise to navigate the complex space of parallelization options'],system,"['Generates execution plans that unify data, operator, and pipeline parallelism', 'Constructs a hierarchical space for massive model-parallel execution plans', 'Designs compilation passes to derive efficient parallel execution plans']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:23,1,,,,Learning from only a few examples or instructions,Few-shot learning,As a target for language models to achieve,2025-02-25T05:02:52.260000000+00:00,Humans ( ability to perform new tasks from few examples),,learning_approach,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:24,1,,,,Training a model on a large corpus of text before fine-tuning,Pre-training on text,As a baseline for fine-tuning,2025-02-25T05:02:52.260000000+00:00,Previous NLP research,,pre-training_method,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:181,1,,,,Trains a deep learning model by dividing it into smaller sub-models that can be trained in parallel,Model-parallel training,Used as a starting point for Alpa's approach,2025-03-12T07:53:25.875000000+00:00,"Various sources (e.g., [10, 22, 49])",,paradigm,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:182,1,,,,A subfield of machine learning that involves training complex neural networks on large datasets,Deep learning (DL),Used as the context for Alpa's approach,2025-03-12T07:53:25.875000000+00:00,"Various sources (e.g., [30, 33])",,field,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:28,3,['Showed that LLaMA-13B outperforms GPT-3 on most benchmarks.'],"['Democratize access and study of LLMs.', 'Can be run on a single GPU.']",2025-02-26T18:47:48.745000000+00:00,"A collection of language models ranging from 7B to 65B parameters, trained on trillions of tokens.",LLaMA,,2025-02-25T05:04:19.565000000+00:00,,[],foundation language models,"['Training state-of-the-art models using publicly available datasets exclusively.', 'Achieving competitive performance with fewer parameters.']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:29,3,['Experimental results showing that training smaller models on more data can lead to better performance.'],['Provides a framework for scaling model and dataset sizes.'],2025-02-26T18:47:48.745000000+00:00,Determine how to best scale the dataset and model sizes for a particular training compute budget.,Scaling laws,,2025-02-25T05:04:19.565000000+00:00,,"['Disregards the inference budget, which becomes critical when serving a language model at scale.']",methodology,"['Introduced by Hoffmann et al. (2022).', 'Ignores the inference budget.']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:30,3,,,2025-03-12T08:05:58.331000000+00:00,AI model that can understand natural language,Large Language Models (LLMs),Used as foundation for Parrot and Semantic Variable,2025-02-26T18:47:48.754000000+00:00,Various LLM papers,,model,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:263,3,['Experiments on various large language and multi-modal models'],"['Efficient execution of LM programs', 'Improved throughput on various tasks']",,A system for efficient execution of complex language model programs,SGLang,,2025-03-12T08:00:49.447000000+00:00,,[],system,"['Frontend language and runtime architecture', 'RadixAttention for KV cache reuse', 'Compressed finite state machines for faster structured output decoding', 'API speculative execution']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:306,3,['Experimental validation on different Llama model sizes and training types'],"['Increased accuracy of early exit at earlier layers', 'Memory footprint is less than other speculative decoding approaches', 'Shared compute and activations of the draft and verification stages']",,End-to-end solution for speeding up inference of large language models,LayerSkip,,2025-03-12T08:05:23.107000000+00:00,,[],inference_solution,"['Layer dropout with low dropout rates for earlier layers and higher dropout rates for later layers', 'Early exit loss where all transformer layers share the same exit', 'Self-speculative decoding solution with early exit at earlier layers and verification with remaining layers']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:264,3,,,,Models that generate text or other outputs,Generative Models,Referenced as components of LM programs,2025-03-12T08:00:49.453000000+00:00,Various papers and research,,models,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:39,17,"['Experimental validation across models and hardware', 'Performance metrics such as decode throughput and end-to-end throughput']","['Significant improvements in inference performance', 'Reduced pipeline bubbles and bubbles-related inefficiencies']",,Efficient large language model inference by piggybacking decodes with chunked prefills,SARATHI,,2025-02-26T18:55:39.620000000+00:00,,[],inference optimization technique,"['Chunked-prefills to improve GPU compute utilization', 'Decode-maximal batching to reduce pipeline bubbles']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:40,17,,,, Prior solution for efficient LLM inference using pipeline parallelism,Orca,Compared to and contrasted with SARATHI,2025-02-26T18:55:39.759000000+00:00,Orca [48],,pipeline parallelism technique,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:49,5,['Evaluations with popular LLMs'],"['Higher throughput', 'Reduced memory usage']",,Custom attention mechanism with reduced computational complexity and improved memory efficiency,PagedAttention,,2025-02-26T19:04:31.913000000+00:00,,[],attention_algorithm,"['Hierarchical token clustering', 'Reduced computational complexity']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:50,5,['Evaluations with popular LLMs'],"['Higher throughput', 'Reduced memory usage']",2025-03-12T07:59:41.418000000+00:00,vLLM library,vLLM,Compared to for serving thousands of LoRA adapters,2025-02-26T19:04:31.913000000+00:00,https://github.com/zihangd/vllm,[],library,"['PagedAttention algorithm', 'Smooth memory growth curve']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:253,5,[],"['Improved throughput by up to 4 times', 'Increased number of served adapters by several orders of magnitude']",,Scalable serving system for thousands of LoRA adapters,S-LoRA,,2025-03-12T07:59:41.411000000+00:00,,[],system,"['Unified Paging', 'Tensor parallelism strategy', 'Highly optimized custom CUDA kernels']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:256,5,,,,Low-Rank Adaptation,LoRA,Allows for efficient fine-tuning of adapters,2025-03-12T07:59:41.418000000+00:00,Hu et al. (2021),,parameter-efficient fine-tuning method,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:257,5,,,,HuggingFace's Parameter-Efficient Fine-Tuning library,HuggingFace PEFT,Compared to for serving thousands of LoRA adapters,2025-03-12T07:59:41.418000000+00:00,https://github.com/huggingface/peft,,library,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:279,4,[],[],,Servings only the decoding phase,Decode-Only,,2025-03-12T08:01:41.303000000+00:00,,"['Impacts overall performance', 'Does not address prefill interferences']",serving_system,[]
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:51,4,,,,Type of models used for LLM serving,LLMs,Serving of LLMs using vLLM,2025-02-26T19:04:31.921000000+00:00,Not mentioned,,large_language_models,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:277,4,"['Evaluation on various popular LLMs and applications', 'Comparison with state-of-the-art systems']","['Improves LLM serving performance', 'Reduces latency requirements', 'Increases cost efficiency']",,Disaggregates prefill and decoding computation for goodput-optimized large language model serving,DistServe,,2025-03-12T08:01:41.303000000+00:00,,[],serving_system,"['Disaggregates prefill and decoding computation', 'Eliminates prefill-decoding interferences', 'Co-optimizes resource allocation and parallelism strategy']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:314,4,,['Enables conventional data flow analysis and optimization'],,Exposes application-level knowledge to public LLM services,Semantic Variable,,2025-03-12T08:05:58.324000000+00:00,,,abstraction,['Provides unified abstraction for application-level knowledge']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:315,4,,,,Applications that communicate with LLMs via natural language,AI Agents or Co-pilots,Used as example application type,2025-03-12T08:05:58.331000000+00:00,Various AI Agents or Co-pilot papers,,application,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:316,4,,,,Applications that use LLMs to generate answers,Chat-based applications,Used as example application type,2025-03-12T08:05:58.331000000+00:00,Various Chat-based application papers,,application,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:82,7,['Experiments on AmoebaNet model and multi-lingual neural machine translation task'],"['Allows efficient scaling of network sizes', 'Reduces memory requirements']",2025-03-12T07:49:52.530000000+00:00,A data parallelization technique developed by Google that enables efficient training of large neural networks on multiple GPUs.,GPipe,used for comparison,2025-03-12T07:46:21.827000000+00:00,GPipe (Google),['Limited to networks that can be expressed as a sequence of layers'],data_parallelization_technique,['Batch-splitting pipelining algorithm']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:83,7,['Experiments on AmoebaNet model and multi-lingual neural machine translation task'],"['Improved efficiency', 'Reduced memory requirements']",,A novel algorithm for efficiently pipelining sub-sequences of layers on separate accelerators.,Batch-splitting pipelining algorithm,,2025-03-12T07:46:21.827000000+00:00,,['Limited to networks that can be pipelined'],algorithm,"['Reduces communication overhead', 'Allows linear speedup when scaling across multiple accelerators']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:84,7,['Experiments on AmoebaNet model and multi-lingual neural machine translation task'],"['Allows efficient scaling of network sizes', 'Reduces memory requirements']",,A technique for scaling neural networks without requiring special algorithms or infrastructure.,Efficient model parallelism,,2025-03-12T07:46:21.827000000+00:00,,['Limited to networks that can be expressed as a sequence of layers'],technique,"['GPipe library', 'Batch-splitting pipelining algorithm']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:85,7,,,,Models that consist of layers that process inputs and produce outputs.,Neural networks,,2025-03-12T07:46:22.020000000+00:00,,,models,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:95,6,,,,Standard solution to language modeling that obtains strong results on multiple benchmarks.,RNNs,Compared to Transformer-XL in terms of capturing long-term dependency and context fragmentation problem.,2025-03-12T07:46:49.343000000+00:00,Not specified in the text.,,Neural Architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:96,6,,,,A type of RNN that uses gradient vanishing and explosion to optimize performance.,LSTMs,Compared to Transformer-XL in terms of capturing long-term dependency and context fragmentation problem.,2025-03-12T07:46:49.343000000+00:00,"Hochreiter and Schmidhuber, 1997",,Neural Architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:97,6,,,,Standard Transformer architecture that has a fixed-length context.,Vanilla Transformers,Compared to Transformer-XL in terms of capturing long-term dependency and context fragmentation problem.,2025-03-12T07:46:49.343000000+00:00,Not specified in the text.,,Neural Architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:117,8,['Analytical analysis on memory requirements and communication volume'],['Allows scaling model size proportional to the number of devices with sustained high efficiency'],,Eliminates memory redundancies in data- and model-parallel training,ZeRO,,2025-03-12T07:48:08.999000000+00:00,,['Requires significant communication between layers in model-parallel training'],optimizer,"['Eliminates memory redundancies', 'Retains low communication volume', 'High computational granularity']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:118,8,,,,Basic parallelization across multiple devices,Data Parallelism (DP),Compared to ZeRO for fundamental limitations in scaling model size,2025-03-12T07:48:09.003000000+00:00,Not specified,,parallelism_approach,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:119,8,,,,Splits the model into smaller parts and trains on different devices,Model Parallelism (MP),"Used to train large models, but MP suffers degradation in efficiency for scaling beyond certain model sizes",2025-03-12T07:48:09.003000000+00:00,Not specified,,parallelism_approach,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:120,8,,,,Chunks the training process and executes different chunks on different devices,Pipeline Parallelism (PP),"Compared to ZeRO, PP makes trade-offs between memory and compute/communication efficiency",2025-03-12T07:48:09.003000000+00:00,Not specified,,parallelism_approach,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:125,18,['Evaluations with 256 GPUs'],"['Near-linear scalability using 256 GPUs', 'Simplified distributed training with minimal intrusion']",,A module that enables distributed data parallel training for deep learning models in PyTorch.,PyTorch Distributed Data Parallel Module,,2025-03-12T07:49:10.036000000+00:00,,[],Deep Learning Library,"['Native support for distributed data parallelism', 'Techniques to accelerate distributed training efficiency: bucketing gradients, overlapping computation with communication, and skipping gradient synchronization']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:126,18,[],"['Simplifies distributed training with minimal intrusion', 'Enables near-linear scalability']",,A technique for parallelizing training data across multiple computational resources.,Distributed Data Parallelism,,2025-03-12T07:49:10.036000000+00:00,,"['Subtle dependencies between computation and communication', 'Requires careful tuning for optimal performance']",Training Technique,[]
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:135,19,,"['efficient parallelization of training process', 'flexible expression of parallel computation patterns']",,A module that provides a lightweight annotation API and an extension to the XLA compiler to support parallel computation.,GShard,,2025-03-12T07:49:52.524000000+00:00,,['requires modifications to XLA compiler and model code'],module,"['flexible way to express parallel computation patterns', 'minimal changes to existing model code']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:139,19,,,,A just-in-time (JIT) compiler developed by Google that allows for efficient execution of neural networks on different hardware platforms.,XLA compiler,used as a base for GShard,2025-03-12T07:49:52.530000000+00:00,XLA compiler (Google),,compiler,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:149,10,"['GPT-2 and BART for table-to-text generation and summarization tasks, respectively']","['obtains comparable performance in the full data setting', 'outperforms adapter-tuning in low-data settings']",,"A lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and optimizes a small task-specific vector (the prex).",Prex-tuning,,2025-03-12T07:50:42.032000000+00:00,,['may not be suitable for tasks requiring significant modifications to the pre-trained model'],approach,"['prex', 'lightweight fine-tuning']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:151,10,"['GPT-2 and BART for table-to-text generation and summarization tasks, respectively']",['obtains comparable performance in the full data setting and outperforms adapter-tuning in low-data settings'],,A method that freezes most of the pretrained parameters and augments the model with small trainable modules (inspired by adapter-tuning).,Lightweight fine-tuning,,2025-03-12T07:50:42.032000000+00:00,,['may not be suitable for tasks requiring significant modifications to the pre-trained model'],method,"['adapter-tuning', 'prompting']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:153,10,,,,A method that inserts additional task-specific layers between the layers of pretrained language models.,Adapter-tuning,['method that is extended and adapted'],2025-03-12T07:50:42.038000000+00:00,"['Rebuffi et al., 2017; Houlsby et al., 2019']",,method,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:158,20,['Experimental results on training iterations with 1 trillion parameters'],"['Improved throughput (10%+)', 'Comparable memory footprint to existing approaches']",,"Combines tensor, pipeline, and data parallelism for efficient large-scale language model training on GPU clusters",Interleaved Pipelining Schedule,,2025-03-12T07:51:21.571000000+00:00,,['Up to 10% overhead due to pipelining schedule'],scheduling technique,['Interleaving parallelization and pipelining for improved throughput']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:160,20,,,,Techniques for efficient large-scale language model training on multiple GPUs,Tensor and Pipeline Parallelism,Techniques used in Megatron-LM for efficient training on GPU clusters,2025-03-12T07:51:21.575000000+00:00,Lapuschkin et al. (2020) and Dal Maso et al. (2019),,model parallelism techniques,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:175,11,,,,Largest single dense model as of Dec 2021,Megatron-Turing NLG 530B,Used as example of large dense model,2025-03-12T07:53:03.097000000+00:00,Not specified,,specific_model,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:172,11,['Experimental comparisons with existing MoE and dense model architectures'],"['Faster inference (up to 4.5x compared to quality-equivalent dense models)', 'Cheaper inference (9x compared to quality-equivalent dense models)']",,End-to-end MoE training and inference solution for large models,DeepSpeed-MoE,,2025-03-12T07:53:03.092000000+00:00,,['Training cost saving demonstrated for encoder-decoder models (5x) but not fully explored for other models'],training_and_inference_solution,"['MoE architecture designs', 'Model compression techniques (up to 3.7x size reduction)', 'Highly optimized inference system (7.3x better latency and cost)']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:174,11,,,,Traditional model architecture requiring high compute resources,Dense model,Used as comparison baseline,2025-03-12T07:53:03.097000000+00:00,Not specified,,model_architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:187,12,[],"['Better accuracy compared to in-context learning', 'Dramatically lower computational costs']",,Alternative paradigm for adapting pre-trained language models to new tasks with fewer resources required,Few-Shot Parameter-Efficient Fine-Tuning,,2025-03-12T07:53:46.845000000+00:00,,[],method,"['Parameter-efficient fine-tuning with adapter modules and prompt tuning', 'Sparse update methods for reducing computational costs']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:188,12,,,,Large-scale language models trained on extensive datasets,Pre-trained Language Models,Used as initialization for downstream tasks,2025-03-12T07:53:46.852000000+00:00,Other papers,,model,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:189,12,,,,Approach for adapting pre-trained language models to new tasks using prompted examples,In-Context Learning,Compared to novel concept for accuracy and computational cost,2025-03-12T07:53:46.852000000+00:00,Other papers,,method,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:196,13,['Experiments on ImageNet and language tasks'],['Improved performance on ImageNet and language tasks'],,A notion of using deeper and narrower transformer configurations for masked autoencoder training,Bamboo,,2025-03-12T07:54:17.485000000+00:00,,[],Transformer configuration,['Deeper and narrower transformer configurations']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:198,13,,,,The original Transformer architecture with self-attention mechanism,Original Transformer,Base architecture that is extended,2025-03-12T07:54:17.561000000+00:00,"Attention Is All You Need (Vaswani et al., 2017)",,Architecture,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:197,13,['Experiments on ImageNet and language tasks'],['Reduced over-smoothing issue in deep transformer training'],,A transformer configuration that uses masked autoencoder training to alleviate over-smoothing issue in deep transformer training,Masked Autoencoder Transformer,,2025-03-12T07:54:17.485000000+00:00,,[],Transformer configuration,['Masked autoencoder training']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:204,2,['Experimental evaluation on various workloads.'],['Improved performance in certain workloads.'],,A method for dynamically adjusting the level of parallelism based on the workload.,Switchable parallelism,,2025-03-12T07:55:17.965000000+00:00,,['Requires careful tuning and optimization.'],parallelism_technique,"['Identifier layers for parallelism strategies.', 'Techniques for adapting parallelism on-the-fly.']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:203,2,['Experimental evaluation on a real-world MoE-based model.'],"['Efficient and effective scaling of MoE models.', 'Ability to adapt to dynamic workloads.']",,A highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining.,TUTEL,,2025-03-12T07:55:17.965000000+00:00,,['Requires careful design and implementation.'],framework,"['Identical layout for distributing MoE model parameters and input data.', 'Switchable parallelism and dynamic pipelining methods.']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:205,2,['Experimental evaluation on various workloads.'],['Improved performance in certain workloads.'],,A method for dynamically reconfiguring the pipelining process.,Dynamic pipelining,,2025-03-12T07:55:17.965000000+00:00,,['Requires careful tuning and optimization.'],pipelining_technique,['Techniques for adapting pipelining on-the-fly.']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:206,2,['Experimental evaluation on MoE workloads.'],['Improved communication efficiency.'],,A technique for efficient communication between experts.,Flexible All-to-All,,2025-03-12T07:55:17.965000000+00:00,,['Requires careful implementation.'],communication_technique,['Improves communication efficiency in MoE.']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:207,2,['Experimental evaluation on MoE workloads.'],['Improved communication efficiency.'],,A two-dimensional hierarchical version of the All-to-All technique.,2DH All-to-All,,2025-03-12T07:55:17.965000000+00:00,,['Requires careful implementation.'],communication_technique,['Improves communication efficiency in MoE.']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:208,2,['Experimental evaluation on MoE workloads.'],['Improved encoding and decoding efficiency.'],,A method for efficient encoding and decoding of expert outputs.,Fast encode/decode,,2025-03-12T07:55:17.965000000+00:00,,['Requires careful implementation.'],encoding_technique,['Improves encoding and decoding efficiency.']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:209,2,,,,A sparse architecture that employs multiple parallel sub-models called experts.,Sparsely-gated Mixture-of-Experts (MoE),Base architecture for TUTEL,2025-03-12T07:55:17.973000000+00:00,"Shazeer et al., 2017",,framework,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:220,14,[],"['Accelerates inference without retraining or architecture changes', 'Maintains identical outputs']",,A method to sample from autoregressive models faster without any changes to the outputs,Speculative Decoding,,2025-03-12T07:55:58.539000000+00:00,,[],inference_method,"['Observation that some subtasks can be approximated well by more efficient models', 'Use of speculative execution and a novel sampling method']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:221,14,,,,Large autoregressive models,Transformers,Target model for acceleration,2025-03-12T07:55:58.546000000+00:00,Vaswani et al. (2017),,model,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:222,14,,,,A type of model used for predicting the next token in a sequence,Autoregressive models,Target of speculative decoding method,2025-03-12T07:55:58.546000000+00:00,General machine learning concept,,class,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:240,21,[],['Improved context length extension'],,A technique that employs a training process inspired by contrastive learning,Focused Transformer (FOT),,2025-03-12T07:57:50.648000000+00:00,,['Risk of distraction issue'],architecture,['Enhanced context length limitation']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:241,21,,,,A training process inspired by self-supervised learning,Contrastive Learning,Inspired the contrastive learning process used in FOT,2025-03-12T07:57:50.653000000+00:00,"""Ho et al. (2020)""",,method,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:244,22,[],"['Develops general-purpose knowledge', 'Improves performance on downstream tasks']",,Pre-training a model on a data-rich task to develop general-purpose abilities and knowledge that can be transferred to downstream tasks,Transfer Learning,,2025-03-12T07:58:44.583000000+00:00,,[],technique,"['Pre-training on a large dataset', 'Fine-tuning on a downstream task after pre-training']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:245,22,[],"['Can be used for multiple language understanding tasks', 'Achieves state-of-the-art results on many benchmarks']",,A unified text-to-text transformer framework that converts all text-based language problems into a text-to-text format,Text-to-Text Transformer,,2025-03-12T07:58:44.583000000+00:00,,[],architecture,"['Unified framework for text-based language problems', 'Uses attention-based models']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:272,23,['Extensive experiments'],"['Ensures fairness in serving LLM requests', 'Achieves a tight upper bound on the service difference between two backlogged clients']",,A fair scheduler based on the continuous batching mechanism,VTC,,2025-03-12T08:01:12.991000000+00:00,,[],Scheduling Algorithm,"['Accounts for the number of input and output tokens processed', 'Introduces the concept of virtual tokens']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:273,23,,,,Simple and widely used scheduling discipline,FCFS,Referenced as a baseline for comparison,2025-03-12T08:01:12.996000000+00:00,Not explicitly mentioned,,Scheduling Discipline,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:286,9,['Experimental evaluation on MT-bench and code completion tasks'],"['Improved decoding efficiency', 'More parallelizable on single or multiple modern accelerators', 'Compatible with concurrent memory-efficient attention']",,"Exact, parallel decoding algorithm that accelerates LLM decoding",LOOKAHEAD DECODING,,2025-03-12T08:02:56.385000000+00:00,,"['Requires careful tuning of hyperparameters', 'May not achieve significant speedups on all tasks']",decoding_algorithm,"['Exact and parallel decoding without using auxiliary models or data stores', 'Trading per-step log(FLOPs) to reduce the number of total decoding steps']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:287,9,,,,Generates text based on one token at a time,Autoregressive decoding,Referenced as inefficient and memory bandwidth bounded,2025-03-12T08:02:56.391000000+00:00,Large language models (LLMs) textbooks,,decoding_method,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:288,9,,,,Uses a draft model to speculate subsequent tokens and then verifies them in parallel,Speculative decoding,Referenced as a method for accelerating LLM decoding,2025-03-12T08:02:56.391000000+00:00,"Speculative Decoding (Chen et al., 2023)",,decoding_method,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:289,9,,,,Improved attention mechanism for parallelizing attention computation,Concurrent memory-efficient attention,Compatible with LOOKAHEAD DECODING,2025-03-12T08:02:56.391000000+00:00,FlashAttention (Open-source),,attention_mechanism,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:299,15,['Experimental evaluation'],"['High throughput', 'High utilization']",,A flexible spatial-temporal multiplexing system for efficient multiple LLM serving,MuxServe,,2025-03-12T08:04:38.211000000+00:00,,[],system,"['Colocation of LLMs', 'Predeployment based on frequency rankings', 'Adaptive batch scheduling strategy']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:300,15,,,,Allocating separate groups of GPUs for each LLM to accommodate their large model size and the key-value cache,Spatial Partitioning,Compared to MuxServe,2025-03-12T08:04:38.216000000+00:00,"Huggingface, 2023; NVIDIA, 2023a; Kwon et al., 2023",,approach,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:301,15,,,,Phases in LLM serving where memory resources can be multiplexed,Prefill and Decoding Phases,MuxServe leverages the characteristics of prefill and decoding phases,2025-03-12T08:04:38.216000000+00:00,Not mentioned,,concept,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:313,24,,['Improves end-to-end performance of LLM-based applications'],,Efficient serving of LLM-based applications,Parrot,,2025-03-12T08:05:58.324000000+00:00,,,system,['Introduces Semantic Variable abstraction']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:317,24,,,,Systems that provide LLM-based APIs,Public LLM services,Used as foundation for Parrot,2025-03-12T08:05:58.331000000+00:00,Various Public LLM service papers,,system,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:323,25,['Experimental evaluation'],"['Competitive performance on image, video, and speech recognition tasks']",,A model for input and output safety.,Llama Guard 3,,2025-03-12T08:07:11.401000000+00:00,,['Still under development'],foundation_model,['Integrated safety features']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:324,25,['Experimental evaluation'],"['Competitive performance on image, video, and speech recognition tasks']",,"A method for integrating image, video, and speech capabilities into Llama 3.",Compositional Approach,,2025-03-12T08:07:11.401000000+00:00,,['Still under development'],method,['Modular architecture']
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:321,26,['Experimental evaluation'],['Comparable quality to leading language models such as GPT-4'],,"A new set of foundation models that natively support multilinguality, coding, reasoning, and tool usage.",Llama 3,,2025-03-12T08:07:11.401000000+00:00,,['Still under development'],foundation_model,"['Multilinguality', 'Coding', 'Reasoning', 'Tool usage']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:325,26,,,,"General models of language, vision, speech, and/or other modalities that are designed to support a large variety of AI tasks.",Foundation Models,Used as a comparison for Llama 3,2025-03-12T08:07:11.407000000+00:00,Not specified,,concept,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:322,27,['Experimental evaluation'],['Comparable quality to leading language models such as GPT-4'],,A specific model in the Llama 3 herd.,Llama 3.1,,2025-03-12T08:07:11.401000000+00:00,,['Still under development'],foundation_model,"['Improved data quality and quantity', 'Careful pre-processing and curation pipelines']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:326,27,,,,Two-stage process for training foundation models.,Pre-training and Post-training,Used to explain Llama 3 development process,2025-03-12T08:07:11.407000000+00:00,Not specified,,concept,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:334,16,['Comprehensive experiments in dense and sparse architectures'],"['Stabilizes training', 'Enhances performance, particularly in LLMs']",,A hybrid normalization strategy that integrates advantages of Pre-Norm and Post-Norm approaches,HybridNorm,,2025-03-12T08:07:38.904000000+00:00,,[],normalization_strategy,"['Employing QKV normalization within attention mechanism and Post-Norm in feed-forward network (FFN)', 'Combining benefits of both Pre-Norm and Post-Norm approaches']"
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:335,16,,,,Normalization strategy used in Pre-Norm structures,Pre-Norm,Used in training due to easier training,2025-03-12T08:07:38.907000000+00:00,"Layer Normalization (Ba et al., 2016)",,normalization_approach,
4:2fcebb07-52d8-475e-9f6c-ef6fb2ecbead:336,16,,,,Normalization strategy used in Post-Norm structures,Post-Norm,Achieved better performance compared to Pre-Norm,2025-03-12T08:07:38.907000000+00:00,"Layer Normalization (Ba et al., 2016)",,normalization_approach,
